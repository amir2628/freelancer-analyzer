"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ GPU –∏ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
"""

import torch
import sys
from pathlib import Path

def check_gpu_availability():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU"""
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU...")
    print(f"PyTorch –≤–µ—Ä—Å–∏—è: {torch.__version__}")
    print(f"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDA –≤–µ—Ä—Å–∏—è: {torch.version.cuda}")
        print(f"cuDNN –≤–µ—Ä—Å–∏—è: {torch.backends.cudnn.version()}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            gpu_props = torch.cuda.get_device_properties(i)
            print(f"\nüì± GPU {i}: {gpu_props.name}")
            print(f"   –ü–∞–º—è—Ç—å: {gpu_props.total_memory / (1024**3):.1f} GB")
            print(f"   Compute Capability: {gpu_props.major}.{gpu_props.minor}")
            print(f"   –ú—É–ª—å—Ç–∏–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä—ã: {gpu_props.multi_processor_count}")
        
        # –¢–µ—Å—Ç –ø—Ä–æ—Å—Ç–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏ –Ω–∞ GPU
        try:
            device = torch.device("cuda:0")
            x = torch.randn(1000, 1000, device=device)
            y = torch.randn(1000, 1000, device=device)
            z = torch.mm(x, y)
            print(f"\n‚úÖ –¢–µ—Å—Ç GPU –æ–ø–µ—Ä–∞—Ü–∏–π: –£—Å–ø–µ—à–Ω–æ")
            print(f"   –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞: {z.shape}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
            memory_allocated = torch.cuda.memory_allocated(0) / (1024**2)
            memory_reserved = torch.cuda.memory_reserved(0) / (1024**2)
            print(f"   –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø–∞–º—è—Ç–∏: {memory_allocated:.1f} MB")
            print(f"   –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ –ø–∞–º—è—Ç–∏: {memory_reserved:.1f} MB")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ GPU: {e}")
            
    else:
        print("‚ùå GPU –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")
        print("\nüîß –í–æ–∑–º–æ–∂–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è:")
        print("1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ CUDA Toolkit: https://developer.nvidia.com/cuda-downloads")
        print("2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ PyTorch —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA:")
        print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
        print("3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥—Ä–∞–π–≤–µ—Ä—ã NVIDIA")

def check_transformers_gpu():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã transformers —Å GPU"""
    print("\nü§ñ –ü—Ä–æ–≤–µ—Ä–∫–∞ Transformers —Å GPU...")
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
        
        model_name = "microsoft/DialoGPT-small"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∞–ª–µ–Ω—å–∫—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∞
        print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        if torch.cuda.is_available():
            print("–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ GPU...")
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                torch_dtype=torch.float16,
                device_map="auto"
            )
            
            pipe = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                device=0,
                max_length=50
            )
            
            # –¢–µ—Å—Ç–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
            test_input = "Hello, how are you"
            result = pipe(test_input, max_length=30, num_return_sequences=1)
            print(f"‚úÖ –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞ GPU —É—Å–ø–µ—à–µ–Ω")
            print(f"   –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç: {test_input}")
            print(f"   –†–µ–∑—É–ª—å—Ç–∞—Ç: {result[0]['generated_text']}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
            memory_allocated = torch.cuda.memory_allocated(0) / (1024**3)
            memory_reserved = torch.cuda.memory_reserved(0) / (1024**3)
            print(f"   –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {memory_allocated:.2f}GB / {memory_reserved:.2f}GB")
            
        else:
            print("‚ö†Ô∏è GPU –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ, —Ç–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ CPU...")
            model = AutoModelForCausalLM.from_pretrained(model_name)
            pipe = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                device=-1,
                max_length=50
            )
            
            test_input = "Hello"
            result = pipe(test_input, max_length=20, num_return_sequences=1)
            print(f"‚úÖ –¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞ CPU —É—Å–ø–µ—à–µ–Ω")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ Transformers: {e}")
        import traceback
        traceback.print_exc()

def check_memory_requirements():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –ø–∞–º—è—Ç–∏"""
    print("\nüíæ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∫ –ø–∞–º—è—Ç–∏...")
    
    models_memory = {
        "microsoft/DialoGPT-small": 0.5,
        "microsoft/DialoGPT-medium": 1.5,
        "microsoft/DialoGPT-large": 3.0,
        "facebook/blenderbot-400M-distill": 1.2
    }
    
    if torch.cuda.is_available():
        available_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        print(f"–î–æ—Å—Ç—É–ø–Ω–∞—è GPU –ø–∞–º—è—Ç—å: {available_memory:.1f} GB")
        
        print("\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –º–æ–¥–µ–ª—è–º:")
        for model, required_memory in models_memory.items():
            if available_memory >= required_memory + 1:  # +1GB –¥–ª—è –±—É—Ñ–µ—Ä–∞
                status = "‚úÖ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è"
            elif available_memory >= required_memory:
                status = "‚ö†Ô∏è –í–æ–∑–º–æ–∂–Ω–æ (–º–∞–ª–æ –±—É—Ñ–µ—Ä–∞)"
            else:
                status = "‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏"
                
            print(f"   {model}: {required_memory}GB —Ç—Ä–µ–±—É–µ—Ç—Å—è - {status}")
    else:
        print("GPU –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ - –≤—Å–µ –º–æ–¥–µ–ª–∏ –±—É–¥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ CPU")

def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏"""
    print("üöÄ –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ GPU –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –∞–Ω–∞–ª–∏–∑–∞ —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤\n")
    
    check_gpu_availability()
    check_transformers_gpu()
    check_memory_requirements()
    
    print("\nüìã –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
    if torch.cuda.is_available():
        available_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        if available_memory >= 4:
            print("‚úÖ –í–∞—à–∞ —Å–∏—Å—Ç–µ–º–∞ –æ—Ç–ª–∏—á–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å GPU")
            print("   –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å DialoGPT-medium –∏–ª–∏ DialoGPT-large")
        elif available_memory >= 2:
            print("‚úÖ –í–∞—à–∞ —Å–∏—Å—Ç–µ–º–∞ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å GPU")
            print("   –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å DialoGPT-medium")
        else:
            print("‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è GPU –ø–∞–º—è—Ç—å")
            print("   –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å DialoGPT-small –∏–ª–∏ —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ CPU")
    else:
        print("‚ÑπÔ∏è –°–∏—Å—Ç–µ–º–∞ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞ CPU")
        print("   –≠—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ, –Ω–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω–µ–µ")
        print("   –î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∫—É CUDA")

if __name__ == "__main__":
    main()
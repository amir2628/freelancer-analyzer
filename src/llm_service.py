"""
–°–µ—Ä–≤–∏—Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª—å—é –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
"""

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Any
import logging
import asyncio
import json
import re

logger = logging.getLogger(__name__)

class LLMService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª—å—é"""
    
    def __init__(self):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –ª–µ–≥–∫—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        self.model_name = "microsoft/DialoGPT-medium"
        # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è fallback
        self.alternative_models = [
            "microsoft/DialoGPT-small",
            "gpt2",
            "distilgpt2"
        ]
        self.tokenizer = None
        self.model = None
        self.pipeline = None
        self.ready = False
        
        # –£–ª—É—á—à–µ–Ω–Ω–∞—è –¥–µ—Ç–µ–∫—Ü–∏—è GPU
        self.device = self._detect_best_device()
        logger.info(f"–í—ã–±—Ä–∞–Ω–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π: {self.device}")
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        self.generation_config = {
            "max_new_tokens": 400,
            "temperature": 0.8,
            "do_sample": True,
            "top_p": 0.9,
            "top_k": 50,
            "repetition_penalty": 1.1,
            "no_repeat_ngram_size": 3,
            "pad_token_id": None  # –£—Å—Ç–∞–Ω–æ–≤–∏–º –ø–æ–∑–∂–µ
        }
        
    def _detect_best_device(self) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª—É—á—à–µ–≥–æ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞"""
        if torch.cuda.is_available():
            gpu_count = torch.cuda.device_count()
            gpu_name = torch.cuda.get_device_name(0) if gpu_count > 0 else "Unknown GPU"
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3) if gpu_count > 0 else 0
            
            logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ GPU: {gpu_name}")
            logger.info(f"–î–æ—Å—Ç—É–ø–Ω–∞—è GPU –ø–∞–º—è—Ç—å: {gpu_memory:.1f} GB")
            logger.info(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö GPU: {gpu_count}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ª–∏ –ø–∞–º—è—Ç–∏ –¥–ª—è –º–æ–¥–µ–ª–∏
            if gpu_memory >= 2.0:  # –ú–∏–Ω–∏–º—É–º 2GB –¥–ª—è DialoGPT-medium
                return "cuda"
            else:
                logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ GPU –ø–∞–º—è—Ç–∏ ({gpu_memory:.1f}GB < 2.0GB), –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")
                return "cpu"
        else:
            logger.info("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU")
            return "cpu"
        
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
        try:
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ {self.model_name}...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
            loop = asyncio.get_event_loop()
            success = await loop.run_in_executor(None, self._load_model)
            
            if success:
                self.ready = True
                logger.info(f"–Ø–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
                
                # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
                await self._test_model()
            else:
                # –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏
                await self._try_alternative_models()
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏: {e}")
            # –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏
            await self._try_alternative_models()
    
    def _load_model(self) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞"""
        try:
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ {self.model_name}...")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            
            # –î–æ–±–∞–≤–ª—è–µ–º pad_token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                logger.info("–î–æ–±–∞–≤–ª–µ–Ω pad_token –≤ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä")
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º pad_token_id –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            self.generation_config["pad_token_id"] = self.tokenizer.eos_token_id
            
            logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {self.model_name} –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ {self.device}...")
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤
            if self.device == "cuda":
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float16,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–æ–≤–∏–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                    device_map="auto",
                    low_cpu_mem_usage=True,
                    trust_remote_code=True
                )
                logger.info("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ GPU —Å –ø–æ–ª–æ–≤–∏–Ω–Ω–æ–π —Ç–æ—á–Ω–æ—Å—Ç—å—é")
            else:
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    torch_dtype=torch.float32,
                    low_cpu_mem_usage=True,
                    trust_remote_code=True
                )
                logger.info("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ CPU")
            
            # –°–æ–∑–¥–∞–µ–º pipeline –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
            device_id = 0 if self.device == "cuda" else -1
            
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device=device_id,
                return_full_text=False,
                clean_up_tokenization_spaces=True,
                truncation=True
            )
            
            logger.info(f"Pipeline —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ: {self.device}")
            
            # –¢–µ—Å—Ç–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç—ã
            if self.device == "cuda":
                self._test_gpu_inference()
            
            return True
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ {self.model_name}: {e}")
            return False
    
    def _test_gpu_inference(self):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏ –Ω–∞ GPU"""
        try:
            test_input = "–¢–µ—Å—Ç GPU"
            with torch.no_grad():
                result = self.pipeline(test_input, **{
                    "max_new_tokens": 20,
                    "temperature": 0.7,
                    "do_sample": True,
                    "pad_token_id": self.tokenizer.eos_token_id
                })
            logger.info("‚úÖ GPU –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –ø–∞–º—è—Ç–∏
            if torch.cuda.is_available():
                memory_allocated = torch.cuda.memory_allocated(0) / (1024**3)
                memory_reserved = torch.cuda.memory_reserved(0) / (1024**3)
                logger.info(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GPU –ø–∞–º—è—Ç–∏: {memory_allocated:.2f}GB –≤—ã–¥–µ–ª–µ–Ω–æ, {memory_reserved:.2f}GB –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ")
                
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è GPU: {e}")
            # –ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ CPU –≤ —Å–ª—É—á–∞–µ –ø—Ä–æ–±–ª–µ–º —Å GPU
            self.device = "cpu"
            self._fallback_to_cpu()
    
    def _fallback_to_cpu(self):
        """–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ CPU –≤ —Å–ª—É—á–∞–µ –ø—Ä–æ–±–ª–µ–º —Å GPU"""
        try:
            logger.warning("–ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ CPU...")
            
            # –û—á–∏—â–∞–µ–º GPU –ø–∞–º—è—Ç—å
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ CPU
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float32,
                low_cpu_mem_usage=True
            )
            
            # –ü–µ—Ä–µ—Å–æ–∑–¥–∞–µ–º pipeline –¥–ª—è CPU
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device=-1,  # CPU
                return_full_text=False,
                clean_up_tokenization_spaces=True,
                truncation=True
            )
            
            logger.info("–£—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–∫–ª—é—á–∏–ª–∏—Å—å –Ω–∞ CPU")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è –Ω–∞ CPU: {e}")
            self.pipeline = None
    
    async def _try_alternative_models(self):
        """–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏"""
        for model_name in self.alternative_models:
            try:
                logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª–∏: {model_name}")
                self.model_name = model_name
                
                loop = asyncio.get_event_loop()
                success = await loop.run_in_executor(None, self._load_model)
                
                if success:
                    self.ready = True
                    logger.info(f"–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –º–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                    await self._test_model()
                    return
                    
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å {model_name}: {e}")
                continue
        
        # –ï—Å–ª–∏ –Ω–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å
        logger.warning("–ù–∏ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–µ–∂–∏–º –±–µ–∑ LLM")
        self.ready = True
        self.pipeline = None
    
    async def _test_model(self):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏"""
        if not self.pipeline:
            logger.warning("Pipeline –Ω–µ —Å–æ–∑–¥–∞–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç –º–æ–¥–µ–ª–∏")
            return
        
        try:
            logger.info("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
            test_prompt = "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?"
            
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None,
                lambda: self.pipeline(test_prompt, max_new_tokens=20, do_sample=False)
            )
            
            if result and len(result) > 0:
                response = result[0].get('generated_text', '')
                logger.info(f"–¢–µ—Å—Ç –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–µ–Ω. –†–µ–∑—É–ª—å—Ç–∞—Ç: {response[:100]}...")
            else:
                logger.warning("–¢–µ—Å—Ç –º–æ–¥–µ–ª–∏ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞")
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
            self.pipeline = None
    
    def is_ready(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–∞"""
        return self.ready
    
    async def analyze_query_intent(self, query: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –∑–∞–ø—Ä–æ—Å–µ"""
        logger.info(f"–ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞: {query}")
        
        # –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ (—Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ LLM)
        query_lower = query.lower()
        intent = self._classify_intent_by_keywords(query_lower)
        
        # –ï—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞ LLM, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞
        if self.pipeline:
            try:
                enhanced_intent = await self._enhance_intent_with_llm(query, intent)
                intent.update(enhanced_intent)
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è LLM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {e}")
        
        return intent
    
    def _classify_intent_by_keywords(self, query: str) -> Dict[str, Any]:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –Ω–∞–º–µ—Ä–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è 10 –≤–æ–ø—Ä–æ—Å–æ–≤"""
        intent = {
            "query_type": "general",
            "entities": [],
            "metrics": [],
            "filters": {},
            "confidence": 0.8,
            "question_id": None
        }
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Ñ—Ä–∞–∑–∞–º
        if "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç" in query and "–≤—ã—à–µ –¥–æ—Ö–æ–¥" in query:
            intent["question_id"] = 1
            intent["query_type"] = "comparison"
            intent["entities"] = ["cryptocurrency"]
            intent["metrics"] = ["earnings"]
            intent["filters"]["payment_method"] = "cryptocurrency"
            
        elif "—Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –¥–æ—Ö–æ–¥" in query and "—Ä–µ–≥–∏–æ–Ω" in query:
            intent["question_id"] = 2
            intent["query_type"] = "distribution"
            intent["entities"] = ["location"]
            intent["metrics"] = ["earnings"]
            
        elif "–ø—Ä–æ—Ü–µ–Ω—Ç" in query and "—ç–∫—Å–ø–µ—Ä—Ç" in query and "100 –ø—Ä–æ–µ–∫—Ç" in query:
            intent["question_id"] = 3
            intent["query_type"] = "percentage"
            intent["entities"] = ["expert", "projects"]
            intent["metrics"] = ["projects_completed"]
            intent["filters"]["skill_level"] = "expert"
            
        elif "—Å—Ä–µ–¥–Ω—è—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø—Ä–æ–µ–∫—Ç" in query and "–æ–ø—ã—Ç" in query:
            intent["question_id"] = 4
            intent["query_type"] = "statistics"
            intent["entities"] = ["experience"]
            intent["metrics"] = ["avg_project_value"]
            
        elif "—Ä–µ–≥–∏–æ–Ω" in query and ("–±–æ–ª—å—à–µ –≤—Å–µ–≥–æ" in query or "–º–µ–Ω—å—à–µ –≤—Å–µ–≥–æ" in query):
            intent["question_id"] = 5
            intent["query_type"] = "distribution"
            intent["entities"] = ["location"]
            intent["metrics"] = ["earnings"]
            
        elif "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ" in query and "–ø—Ä–æ–µ–∫—Ç" in query and "–≤–ª–∏—è–µ—Ç" in query and "–¥–æ—Ö–æ–¥" in query:
            intent["question_id"] = 6
            intent["query_type"] = "correlation"
            intent["entities"] = ["projects"]
            intent["metrics"] = ["earnings", "projects_completed"]
            
        elif "–ø—Ä–æ—Ü–µ–Ω—Ç" in query and "—Å–ø–æ—Å–æ–± –æ–ø–ª–∞—Ç—ã" in query:
            intent["question_id"] = 7
            intent["query_type"] = "percentage"
            intent["entities"] = ["payment_method"]
            intent["metrics"] = ["payment_method"]
            
        elif "—Ä–∞–∑–Ω–∏—Ü–∞" in query and "–Ω–æ–≤–∏—á–∫" in query and "—ç–∫—Å–ø–µ—Ä—Ç" in query and "—Ä–µ–≥–∏–æ–Ω" in query:
            intent["question_id"] = 8
            intent["query_type"] = "multifactor"
            intent["entities"] = ["expert", "beginner", "location"]
            intent["metrics"] = ["earnings"]
            
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–ª—É—á—à–µ–Ω–Ω–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–æ–ø—Ä–æ—Å–∞ 9 –æ –ø–æ—á–∞—Å–æ–≤–æ–π —Å—Ç–∞–≤–∫–µ
        elif (("—Å—Ä–µ–¥–Ω—è—è –ø–æ—á–∞—Å–æ–≤–∞—è —Å—Ç–∞–≤–∫–∞" in query or "–ø–æ—á–∞—Å–æ–≤–∞—è —Å—Ç–∞–≤–∫–∞" in query or "—Å—Ç–∞–≤–∫–∞" in query) 
              and "—Å–ø–æ—Å–æ–± –æ–ø–ª–∞—Ç—ã" in query):
            intent["question_id"] = 9
            intent["query_type"] = "hourly_rate_analysis"  # –ò–∑–º–µ–Ω–µ–Ω —Ç–∏–ø –∑–∞–ø—Ä–æ—Å–∞
            intent["entities"] = ["payment_method"]
            intent["metrics"] = ["hourly_rate"]
            
        elif "—Å–∫–æ–ª—å–∫–æ" in query and "–ø—Ä–æ–µ–∫—Ç" in query and "–∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü" in query:
            intent["question_id"] = 10
            intent["query_type"] = "statistics"
            intent["entities"] = ["skill_level"]
            intent["metrics"] = ["projects_completed"]
        
        # –ï—Å–ª–∏ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â—É—é –ª–æ–≥–∏–∫—É
        if intent["question_id"] is None:
            # –û–±—â–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ –∑–∞–ø—Ä–æ—Å–∞
            if any(word in query for word in ["—Å—Ä–∞–≤–Ω–∏—Ç—å", "—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ", "–≤—ã—à–µ", "–Ω–∏–∂–µ", "—Ä–∞–∑–Ω–∏—Ü–∞", "–±–æ–ª—å—à–µ", "–º–µ–Ω—å—à–µ"]):
                intent["query_type"] = "comparison"
                
            elif any(word in query for word in ["—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ", "–ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º", "–ø–æ –æ–±–ª–∞—Å—Ç—è–º", "–≥–µ–æ–≥—Ä–∞—Ñ–∏—è", "—Ä–µ–≥–∏–æ–Ω"]):
                intent["query_type"] = "distribution"
                
            elif any(word in query for word in ["–ø—Ä–æ—Ü–µ–Ω—Ç", "%", "–¥–æ–ª—è", "—Å–∫–æ–ª—å–∫–æ", "–∫–∞–∫–æ–π –ø—Ä–æ—Ü–µ–Ω—Ç"]):
                intent["query_type"] = "percentage"
                
            elif any(word in query for word in ["—Å—Ä–µ–¥–Ω–∏–π", "—Å—Ä–µ–¥–Ω–µ–µ", "–º–µ–¥–∏–∞–Ω–∞", "—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞"]):
                intent["query_type"] = "statistics"
                
            # –î–û–ë–ê–í–õ–ï–ù–û: –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–æ—á–∞—Å–æ–≤—ã—Ö —Å—Ç–∞–≤–æ–∫ –≤ –æ–±—â–µ–º —Å–ª—É—á–∞–µ
            elif any(word in query for word in ["–ø–æ—á–∞—Å–æ–≤–∞—è", "—Å—Ç–∞–≤–∫–∞", "—á–∞—Å", "hourly"]):
                intent["query_type"] = "hourly_rate_analysis"
                intent["metrics"].append("hourly_rate")
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
            entities = []
            
            # –°–ø–æ—Å–æ–±—ã –æ–ø–ª–∞—Ç—ã
            if any(word in query for word in ["–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç", "–∫—Ä–∏–ø—Ç–æ", "cryptocurrency", "bitcoin"]):
                entities.append("cryptocurrency")
                intent["filters"]["payment_method"] = "cryptocurrency"
                
            if any(word in query for word in ["–±–∞–Ω–∫", "bank", "–ø–µ—Ä–µ–≤–æ–¥", "transfer"]):
                entities.append("bank_transfer")
                intent["filters"]["payment_method"] = "bank"
                
            if any(word in query for word in ["paypal", "–ø—ç–π–ø–∞–ª"]):
                entities.append("paypal")
                intent["filters"]["payment_method"] = "paypal"
            
            # –£—Ä–æ–≤–Ω–∏ —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—ã
            if any(word in query for word in ["—ç–∫—Å–ø–µ—Ä—Ç", "—ç–∫—Å–ø–µ—Ä—Ç–æ–≤", "expert", "–ø—Ä–æ–¥–≤–∏–Ω—É—Ç", "–æ–ø—ã—Ç–Ω"]):
                entities.append("expert")
                intent["filters"]["skill_level"] = "expert"
                
            if any(word in query for word in ["–Ω–æ–≤–∏—á–æ–∫", "beginner", "–Ω–∞—á–∏–Ω–∞—é—â", "junior"]):
                entities.append("beginner")
                intent["filters"]["skill_level"] = "beginner"
            
            # –†–µ–≥–∏–æ–Ω—ã/–ª–æ–∫–∞—Ü–∏–∏
            if any(word in query for word in ["—Ä–µ–≥–∏–æ–Ω", "—Ä–µ–≥–∏–æ–Ω–æ–≤", "–æ–±–ª–∞—Å—Ç—å", "—Å—Ç—Ä–∞–Ω–∞", "location", "–≥–µ–æ–≥—Ä–∞—Ñ–∏—è"]):
                entities.append("location")
                
            # –ü—Ä–æ–µ–∫—Ç—ã
            if any(word in query for word in ["–ø—Ä–æ–µ–∫—Ç", "–ø—Ä–æ–µ–∫—Ç–æ–≤", "—Ä–∞–±–æ—Ç", "–∑–∞–∫–∞–∑"]):
                entities.append("projects")
                
            intent["entities"] = entities
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
            metrics = []
            if any(word in query for word in ["–¥–æ—Ö–æ–¥", "–∑–∞—Ä–∞–±–æ—Ç–æ–∫", "–∑–∞—Ä–ø–ª–∞—Ç", "earnings", "–¥–µ–Ω—å–≥–∏", "–æ–ø–ª–∞—Ç"]):
                metrics.append("earnings")
                
            if any(word in query for word in ["–ø—Ä–æ–µ–∫—Ç", "—Ä–∞–±–æ—Ç", "–∑–∞–∫–∞–∑", "completed"]):
                metrics.append("projects_completed")
                
            if any(word in query for word in ["—Ä–µ–π—Ç–∏–Ω–≥", "rating", "–æ—Ü–µ–Ω–∫"]):
                metrics.append("rating")
                
            if any(word in query for word in ["—á–∞—Å", "hourly", "—Å—Ç–∞–≤–∫", "–ø–æ—á–∞—Å–æ–≤"]):
                metrics.append("hourly_rate")
                
            intent["metrics"] = metrics
        
        logger.info(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω intent: question_id={intent.get('question_id')}, query_type={intent.get('query_type')}")
        return intent
    
    async def _enhance_intent_with_llm(self, query: str, base_intent: Dict[str, Any]) -> Dict[str, Any]:
        """–£–ª—É—á—à–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é LLM"""
        if not self.pipeline:
            return {}
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏—è
        prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤–æ–ø—Ä–æ—Å –æ —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–∞—Ö –∏ –æ–ø—Ä–µ–¥–µ–ª–∏:
1. –¢–∏–ø –∞–Ω–∞–ª–∏–∑–∞: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, –ø—Ä–æ—Ü–µ–Ω—Ç
2. –ö–ª—é—á–µ–≤—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏: —Ä–µ–≥–∏–æ–Ω, —Å–ø–æ—Å–æ–± –æ–ø–ª–∞—Ç—ã, —É—Ä–æ–≤–µ–Ω—å —ç–∫—Å–ø–µ—Ä—Ç–∏–∑—ã, –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞
3. –ú–µ—Ç—Ä–∏–∫–∏: –¥–æ—Ö–æ–¥, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–µ–∫—Ç–æ–≤, —Ä–µ–π—Ç–∏–Ω–≥

–í–æ–ø—Ä–æ—Å: "{query}"

–û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫—Ä–∞—Ç–∫–∏–º –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º."""
        
        try:
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None, 
                lambda: self.pipeline(prompt, **{
                    "max_new_tokens": 100,
                    "temperature": 0.3,
                    "do_sample": True,
                    "pad_token_id": self.tokenizer.eos_token_id
                })
            )
            
            if response and len(response) > 0:
                generated_text = response[0].get('generated_text', '').strip()
                # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –ø–æ–ª–µ–∑–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM
                enhanced_data = self._extract_intent_from_response(generated_text)
                return enhanced_data if enhanced_data else {}
            
            return {}
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ LLM –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞–º–µ—Ä–µ–Ω–∏—è: {e}")
            return {}
    
    def _extract_intent_from_response(self, response: str) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –æ—Ç–≤–µ—Ç–∞
            enhanced = {}
            
            response_lower = response.lower()
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –∞–Ω–∞–ª–∏–∑–∞
            if "—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ" in response_lower or "comparison" in response_lower:
                enhanced["llm_query_type"] = "comparison"
            elif "—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞" in response_lower or "statistics" in response_lower:
                enhanced["llm_query_type"] = "statistics"
            elif "—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ" in response_lower or "distribution" in response_lower:
                enhanced["llm_query_type"] = "distribution"
            elif "–ø—Ä–æ—Ü–µ–Ω—Ç" in response_lower or "percentage" in response_lower:
                enhanced["llm_query_type"] = "percentage"
            
            return enhanced if enhanced else None
        except:
            return None
    
    async def generate_answer(self, query: str, data_analysis: Dict[str, Any]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö"""
        
        # –ï—Å–ª–∏ LLM –¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        if self.pipeline:
            try:
                logger.info("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º LLM...")
                llm_answer = await self._generate_intelligent_answer(query, data_analysis)
                if llm_answer and len(llm_answer.strip()) > 50:
                    logger.info("LLM —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª –æ—Ç–≤–µ—Ç")
                    return llm_answer
                else:
                    logger.warning("LLM —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–ª —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LLM –æ—Ç–≤–µ—Ç–∞: {e}")
        
        # Fallback –Ω–∞ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –æ—Ç–≤–µ—Ç
        logger.info("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ fallback")
        return self._generate_analytical_fallback(query, data_analysis)
    
    async def _generate_intelligent_answer(self, query: str, data_analysis: Dict[str, Any]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ —Å LLM"""
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∞–Ω–Ω—ã—Ö
        data_context = self._prepare_data_context(data_analysis)
        
        # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        prompt = self._create_analysis_prompt(query, data_context)
        
        try:
            logger.info(f"–û—Ç–ø—Ä–∞–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞ –≤ LLM (–¥–ª–∏–Ω–∞: {len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤)")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None,
                lambda: self._safe_generate(prompt)
            )
            
            if result:
                # –û—á–∏—â–∞–µ–º –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
                cleaned_answer = self._clean_llm_response(result, query)
                if cleaned_answer and len(cleaned_answer) > 50:
                    logger.info(f"LLM –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω (–¥–ª–∏–Ω–∞: {len(cleaned_answer)} —Å–∏–º–≤–æ–ª–æ–≤)")
                    return cleaned_answer
            
            return None
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞: {e}")
            return None
    
    def _safe_generate(self, prompt: str) -> str:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            result = self.pipeline(
                prompt,
                max_new_tokens=self.generation_config["max_new_tokens"],
                temperature=self.generation_config["temperature"],
                do_sample=self.generation_config["do_sample"],
                top_p=self.generation_config["top_p"],
                top_k=self.generation_config["top_k"],
                repetition_penalty=self.generation_config["repetition_penalty"],
                no_repeat_ngram_size=self.generation_config["no_repeat_ngram_size"],
                pad_token_id=self.generation_config["pad_token_id"]
            )
            
            if result and len(result) > 0:
                return result[0].get('generated_text', '')
            
            return None
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ safe_generate: {e}")
            return None
    
    def _create_analysis_prompt(self, query: str, data_context: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö"""
        
        prompt = f"""–¢—ã –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö –æ —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–∞—Ö. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–í–û–ü–†–û–°: {query}

–î–ê–ù–ù–´–ï –î–õ–Ø –ê–ù–ê–õ–ò–ó–ê:
{data_context}

–ò–ù–°–¢–†–£–ö–¶–ò–ò:
1. –í–Ω–∏–º–∞—Ç–µ–ª—å–Ω–æ –∏–∑—É—á–∏ –¥–∞–Ω–Ω—ã–µ
2. –ù–∞–π–¥–∏ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏ —Ç—Ä–µ–Ω–¥—ã
3. –î–∞–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç —Å —á–∏—Å–ª–∞–º–∏
4. –°–¥–µ–ª–∞–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã–≤–æ–¥—ã
5. –°—Ä–∞–≤–Ω–∏ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
6. –û—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
7. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º

–ê–ù–ê–õ–ò–ó:"""
        
        return prompt
    
    def _prepare_data_context(self, data_analysis: Dict[str, Any]) -> str:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è LLM"""
        context_parts = []
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if "summary" in data_analysis:
            summary = data_analysis["summary"]
            context_parts.append(f"""–û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:
- –í—Å–µ–≥–æ —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤: {summary.get('total_records', 0)}
- –°—Ä–µ–¥–Ω–∏–π –¥–æ—Ö–æ–¥: ${summary.get('mean_earnings', 0):.2f}
- –ú–µ–¥–∏–∞–Ω–Ω—ã–π –¥–æ—Ö–æ–¥: ${summary.get('median_earnings', 0):.2f}
- –î–∏–∞–ø–∞–∑–æ–Ω: ${summary.get('min_earnings', 0):.2f} - ${summary.get('max_earnings', 0):.2f}""")
        
        # –î–û–ë–ê–í–õ–ï–ù–û: –ü–æ—á–∞—Å–æ–≤—ã–µ —Å—Ç–∞–≤–∫–∏ –ø–æ —Å–ø–æ—Å–æ–±–∞–º –æ–ø–ª–∞—Ç—ã
        if "hourly_rate_by_payment" in data_analysis:
            hourly_data = data_analysis["hourly_rate_by_payment"]
            if "mean" in hourly_data:
                context_parts.append("–ü–û–ß–ê–°–û–í–´–ï –°–¢–ê–í–ö–ò –ü–û –°–ü–û–°–û–ë–ê–ú –û–ü–õ–ê–¢–´:")
                sorted_methods = sorted(hourly_data["mean"].items(), key=lambda x: x[1], reverse=True)
                for method, rate in sorted_methods:
                    count = hourly_data.get("count", {}).get(method, "?")
                    context_parts.append(f"- {method}: ${rate:.2f}/—á–∞—Å (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {count})")
        
        # –î–æ—Ö–æ–¥—ã –ø–æ —Å–ø–æ—Å–æ–±–∞–º –æ–ø–ª–∞—Ç—ã
        if "earnings_by_payment_method" in data_analysis:
            earnings_data = data_analysis["earnings_by_payment_method"]
            if "mean" in earnings_data:
                context_parts.append("–î–û–•–û–î–´ –ü–û –°–ü–û–°–û–ë–ê–ú –û–ü–õ–ê–¢–´:")
                sorted_methods = sorted(earnings_data["mean"].items(), key=lambda x: x[1], reverse=True)
                for method, income in sorted_methods:
                    count = earnings_data.get("count", {}).get(method, "?")
                    context_parts.append(f"- {method}: ${income:.2f} (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {count})")
        
        # –î–æ—Ö–æ–¥—ã –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º
        if "regional_earnings" in data_analysis:
            regional_data = data_analysis["regional_earnings"]
            if "mean" in regional_data:
                context_parts.append("–î–û–•–û–î–´ –ü–û –†–ï–ì–ò–û–ù–ê–ú:")
                sorted_regions = sorted(regional_data["mean"].items(), key=lambda x: x[1], reverse=True)
                for region, income in sorted_regions[:8]:  # –¢–æ–ø 8
                    count = regional_data.get("count", {}).get(region, "?")
                    context_parts.append(f"- {region}: ${income:.2f} (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {count})")
        
        # –î–æ—Ö–æ–¥—ã –ø–æ —É—Ä–æ–≤–Ω—é –Ω–∞–≤—ã–∫–æ–≤
        if "experience_earnings" in data_analysis:
            exp_data = data_analysis["experience_earnings"]
            if "mean" in exp_data:
                context_parts.append("–î–û–•–û–î–´ –ü–û –£–†–û–í–ù–Æ –ù–ê–í–´–ö–û–í:")
                sorted_skills = sorted(exp_data["mean"].items(), key=lambda x: x[1], reverse=True)
                for skill, income in sorted_skills:
                    count = exp_data.get("count", {}).get(skill, "?")
                    context_parts.append(f"- {skill}: ${income:.2f} (–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {count})")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
        if "total_experts" in data_analysis:
            experts_under_100 = data_analysis.get("experts_under_100_projects", 0)
            percentage = data_analysis.get("percentage_under_100_projects", 0)
            total_experts = data_analysis.get("total_experts", 0)
            
            context_parts.append(f"""–°–¢–ê–¢–ò–°–¢–ò–ö–ê –≠–ö–°–ü–ï–†–¢–û–í:
- –í—Å–µ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤: {total_experts}
- –° –º–µ–Ω–µ–µ 100 –ø—Ä–æ–µ–∫—Ç–∞–º–∏: {experts_under_100} ({percentage:.1f}%)
- –°–æ 100+ –ø—Ä–æ–µ–∫—Ç–∞–º–∏: {total_experts - experts_under_100} ({100-percentage:.1f}%)""")
        
        return "\n\n".join(context_parts) if context_parts else "–î–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã"
    
    def _clean_llm_response(self, response: str, original_query: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ LLM"""
        try:
            cleaned = response.strip()
            
            # –£–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
            if "–ê–ù–ê–õ–ò–ó:" in cleaned:
                parts = cleaned.split("–ê–ù–ê–õ–ò–ó:")
                if len(parts) > 1:
                    cleaned = parts[-1].strip()
            
            # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã
            prefixes_to_remove = ["–û—Ç–≤–µ—Ç:", "–û–¢–í–ï–¢:", "–ê–Ω–∞–ª–∏–∑:", "–ê–Ω–∞–ª–∏—Ç–∏–∫:"]
            for prefix in prefixes_to_remove:
                if cleaned.startswith(prefix):
                    cleaned = cleaned[len(prefix):].strip()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É
            if len(cleaned) < 30:
                return None
            
            # –û–±—Ä–µ–∑–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã
            if len(cleaned) > 1500:
                # –ù–∞—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–Ω–µ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –ª–∏–º–∏—Ç–∞
                sentences = cleaned[:1500].split('.')
                if len(sentences) > 1:
                    cleaned = '.'.join(sentences[:-1]) + '.'
                else:
                    cleaned = cleaned[:1500] + "..."
            
            return cleaned
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –æ—Ç–≤–µ—Ç–∞ LLM: {e}")
            return None
    
    def _generate_analytical_fallback(self, query: str, data_analysis: Dict[str, Any]) -> str:
        """–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π fallback –±–µ–∑ LLM —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –≤—Å–µ—Ö 10 –≤–æ–ø—Ä–æ—Å–æ–≤"""
        
        if not data_analysis:
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∞—à–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞."
        
        query_lower = query.lower()
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–º —Ç–∏–ø–∞–º –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ
        
        # –í–æ–ø—Ä–æ—Å 4: –°—Ä–µ–¥–Ω—è—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø—Ä–æ–µ–∫—Ç–∞ –ø–æ –æ–ø—ã—Ç—É
        if "avg_project_value_by_skill" in data_analysis:
            return self._analyze_avg_project_value(data_analysis)
        
        # –í–æ–ø—Ä–æ—Å 6: –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–æ–µ–∫—Ç–æ–≤ –∏ –¥–æ—Ö–æ–¥–æ–≤
        elif "correlation_analysis" in data_analysis:
            return self._analyze_correlation(data_analysis)
        
        # –í–æ–ø—Ä–æ—Å 7: –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–æ–≤ –æ–ø–ª–∞—Ç—ã
        elif "payment_method_distribution" in data_analysis:
            return self._analyze_payment_distribution(data_analysis)
        
        # –í–æ–ø—Ä–æ—Å 8: –ú–Ω–æ–≥–æ—Ñ–∞–∫—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        elif "multifactor_earnings" in data_analysis:
            return self._analyze_multifactor(data_analysis)
        
        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –í–æ–ø—Ä–æ—Å 9: –ü–æ—á–∞—Å–æ–≤—ã–µ —Å—Ç–∞–≤–∫–∏ –ø–æ —Å–ø–æ—Å–æ–±–∞–º –æ–ø–ª–∞—Ç—ã
        elif "hourly_rate_by_payment" in data_analysis:
            return self._analyze_hourly_rates(data_analysis)
        
        # –í–æ–ø—Ä–æ—Å 10: –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏
        elif "activity_by_qualification" in data_analysis:
            return self._analyze_activity_by_qualification(data_analysis)
        
        # –û—Å—Ç–∞–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é –ª–æ–≥–∏–∫—É
        elif "—ç–∫—Å–ø–µ—Ä—Ç" in query_lower and "–ø—Ä–æ—Ü–µ–Ω—Ç" in query_lower:
            return self._analyze_expert_percentage(data_analysis)
        
        elif "—Å–≤—è–∑—å" in query_lower or "–≤–ª–∏—è" in query_lower:
            return self._analyze_relationships(data_analysis, query)
        
        elif "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç" in query_lower:
            return self._analyze_crypto_comparison(data_analysis)
        
        elif any(word in query_lower for word in ["—Ä–µ–≥–∏–æ–Ω", "–≥–µ–æ–≥—Ä–∞—Ñ–∏—è", "—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ"]):
            return self._analyze_regional_distribution(data_analysis)
        
        else:
            return self._generate_general_analysis(data_analysis, query)
    
    def _analyze_avg_project_value(self, data_analysis: Dict[str, Any]) -> str:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ä–µ–¥–Ω–µ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–µ–∫—Ç–∞ –ø–æ —É—Ä–æ–≤–Ω—é –æ–ø—ã—Ç–∞ (–í–æ–ø—Ä–æ—Å 4)"""
        
        avg_value_data = data_analysis.get("avg_project_value_by_skill", {})
        summary = data_analysis.get("summary", {})
        
        if not avg_value_data or "mean" not in avg_value_data:
            return "–î–∞–Ω–Ω—ã–µ –æ —Å—Ä–µ–¥–Ω–µ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–µ–∫—Ç–æ–≤ –ø–æ —É—Ä–æ–≤–Ω—é –æ–ø—ã—Ç–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."
        
        result = f"""üí∞ **–°—Ä–µ–¥–Ω—è—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø—Ä–æ–µ–∫—Ç–∞ –ø–æ —É—Ä–æ–≤–Ω—é –æ–ø—ã—Ç–∞:**

üìä **–ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞** ({summary.get('total_records', 0)} —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤):
‚Ä¢ –û–±—â–∏–π —Å—Ä–µ–¥–Ω–∏–π –¥–æ—Ö–æ–¥: ${summary.get('mean_earnings', 0):.2f}

üíº **–°—Ç–æ–∏–º–æ—Å—Ç—å –ø—Ä–æ–µ–∫—Ç–∞ –ø–æ —É—Ä–æ–≤–Ω—è–º –Ω–∞–≤—ã–∫–æ–≤:**
"""
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ä–µ–¥–Ω–µ–π —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–µ–∫—Ç–∞
        sorted_skills = sorted(avg_value_data["mean"].items(), key=lambda x: x[1], reverse=True)
        
        for skill, avg_value in sorted_skills:
            count = avg_value_data.get("count", {}).get(skill, 0)
            median_value = avg_value_data.get("median", {}).get(skill, avg_value)
            
            if not pd.isna(avg_value):
                result += f"‚Ä¢ **{skill.capitalize()}**: ${avg_value:.2f} (–º–µ–¥–∏–∞–Ω–∞: ${median_value:.2f}) ‚Äî {count} —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤\n"
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–±—Ä–æ—Å–∞
        if len(sorted_skills) >= 2:
            highest = sorted_skills[0]
            lowest = sorted_skills[-1]
            
            if not pd.isna(highest[1]) and not pd.isna(lowest[1]):
                difference = highest[1] - lowest[1]
                percentage_diff = (difference / lowest[1] * 100) if lowest[1] > 0 else 0
                
                result += f"\nüí° **–ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã:**\n"
                result += f"‚Ä¢ –ù–∞–∏–≤—ã—Å—à–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø—Ä–æ–µ–∫—Ç–∞ —É **{highest[0]}**: ${highest[1]:.2f}\n"
                result += f"‚Ä¢ –ù–∞–∏–º–µ–Ω—å—à–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø—Ä–æ–µ–∫—Ç–∞ —É **{lowest[0]}**: ${lowest[1]:.2f}\n"
                result += f"‚Ä¢ –†–∞–∑–Ω–∏—Ü–∞ –≤ —Å—Ç–æ–∏–º–æ—Å—Ç–∏: ${difference:.2f} ({percentage_diff:.1f}%)\n"
                result += f"‚Ä¢ –û–ø—ã—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ —Å—Ç–æ–∏–º–æ—Å—Ç—å –ø—Ä–æ–µ–∫—Ç–æ–≤"
        
        return result
    
    def _analyze_correlation(self, data_analysis: Dict[str, Any]) -> str:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –ø—Ä–æ–µ–∫—Ç–∞–º–∏ –∏ –¥–æ—Ö–æ–¥–æ–º (–í–æ–ø—Ä–æ—Å 6)"""
        
        correlation_data = data_analysis.get("correlation_analysis", {})
        summary = data_analysis.get("summary", {})
        
        if not correlation_data:
            return "–î–∞–Ω–Ω—ã–µ –æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –º–µ–∂–¥—É –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–æ–µ–∫—Ç–æ–≤ –∏ –¥–æ—Ö–æ–¥–æ–º –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."
        
        correlation_coef = correlation_data.get("correlation_coefficient", 0)
        earnings_by_range = correlation_data.get("earnings_by_project_range", {})
        
        result = f"""üìà **–í–ª–∏—è–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–µ–∫—Ç–æ–≤ –Ω–∞ –¥–æ—Ö–æ–¥ —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–∞:**

üîç **–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑**:
‚Ä¢ –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏: {correlation_coef:.3f}
‚Ä¢ –°–∏–ª–∞ —Å–≤—è–∑–∏: {self._interpret_correlation(correlation_coef)}

üìä **–î–æ—Ö–æ–¥—ã –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º –ø—Ä–æ–µ–∫—Ç–æ–≤:**
"""
        
        if earnings_by_range and "mean" in earnings_by_range:
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω—ã –ª–æ–≥–∏—á–µ—Å–∫–∏
            range_order = ['1-10', '11-50', '51-100', '101-500', '500+']
            
            for range_name in range_order:
                if range_name in earnings_by_range["mean"]:
                    avg_earnings = earnings_by_range["mean"][range_name]
                    count = earnings_by_range.get("count", {}).get(range_name, 0)
                    
                    if not pd.isna(avg_earnings):
                        vs_overall = avg_earnings - summary.get('mean_earnings', 0)
                        result += f"‚Ä¢ **{range_name} –ø—Ä–æ–µ–∫—Ç–æ–≤**: ${avg_earnings:.2f} ({vs_overall:+.0f}$ –∫ —Å—Ä–µ–¥–Ω–µ–º—É) ‚Äî {count} —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤\n"
        
        result += f"\nüí° **–í—ã–≤–æ–¥—ã:**\n"
        
        if correlation_coef > 0.3:
            result += "‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–µ–∫—Ç–æ–≤ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –¥–æ—Ö–æ–¥—ã\n"
            result += "‚Ä¢ –ß–µ–º –±–æ–ª—å—à–µ –ø—Ä–æ–µ–∫—Ç–æ–≤, —Ç–µ–º –≤—ã—à–µ —Å—Ä–µ–¥–Ω–∏–π –¥–æ—Ö–æ–¥\n"
            result += "‚Ä¢ –û–ø—ã—Ç –∏ —Ä–µ–ø—É—Ç–∞—Ü–∏—è —Ä–∞—Å—Ç—É—Ç —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö —Ä–∞–±–æ—Ç"
        elif correlation_coef > 0.1:
            result += "‚Ä¢ –°–ª–∞–±–∞—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è —Å–≤—è–∑—å –º–µ–∂–¥—É –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–æ–µ–∫—Ç–æ–≤ –∏ –¥–æ—Ö–æ–¥–æ–º\n"
            result += "‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–µ–∫—Ç–æ–≤ –º–æ–∂–µ—Ç –±—ã—Ç—å –≤–∞–∂–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞"
        else:
            result += "‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–µ–∫—Ç–æ–≤ —Å–ª–∞–±–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –¥–æ—Ö–æ–¥—ã\n"
            result += "‚Ä¢ –î—Ä—É–≥–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã (–Ω–∞–≤—ã–∫–∏, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è) –±–æ–ª–µ–µ –≤–∞–∂–Ω—ã"
        
        return result
    
    def _analyze_payment_distribution(self, data_analysis: Dict[str, Any]) -> str:
        """–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–ø–æ—Å–æ–±–æ–≤ –æ–ø–ª–∞—Ç—ã (–í–æ–ø—Ä–æ—Å 7)"""
        
        distribution_data = data_analysis.get("payment_method_distribution", {})
        summary = data_analysis.get("summary", {})
        
        if not distribution_data:
            return "–î–∞–Ω–Ω—ã–µ –æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —Å–ø–æ—Å–æ–±–æ–≤ –æ–ø–ª–∞—Ç—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."
        
        counts = distribution_data.get("counts", {})
        percentages = distribution_data.get("percentages", {})
        
        result = f"""üí≥ **–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø–æ—Å–æ–±–æ–≤ –æ–ø–ª–∞—Ç—ã —Å—Ä–µ–¥–∏ —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤:**

üìä **–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞** ({summary.get('total_records', 0)} —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤):

**–ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å —Å–ø–æ—Å–æ–±–æ–≤ –æ–ø–ª–∞—Ç—ã:**
"""
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏
        if percentages:
            sorted_methods = sorted(percentages.items(), key=lambda x: x[1], reverse=True)
            
            for i, (method, percentage) in enumerate(sorted_methods, 1):
                count = counts.get(method, 0)
                emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else "üî∏"
                
                result += f"{emoji} **{method}**: {percentage:.1f}% ({count} —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤)\n"
        
        # –ê–Ω–∞–ª–∏–∑ –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        if percentages:
            most_popular = max(percentages.items(), key=lambda x: x[1])
            least_popular = min(percentages.items(), key=lambda x: x[1])
            
            result += f"\nüí° **–ê–Ω–∞–ª–∏–∑ –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç–∏:**\n"
            result += f"‚Ä¢ –°–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π: **{most_popular[0]}** ({most_popular[1]:.1f}%)\n"
            result += f"‚Ä¢ –ù–∞–∏–º–µ–Ω–µ–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–π: **{least_popular[0]}** ({least_popular[1]:.1f}%)\n"
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—é
            top_two = sorted(percentages.values(), reverse=True)[:2]
            top_two_share = sum(top_two)
            
            if top_two_share > 70:
                result += f"‚Ä¢ –í—ã—Å–æ–∫–∞—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è: –¥–≤–∞ –ª–∏–¥–∏—Ä—É—é—â–∏—Ö —Å–ø–æ—Å–æ–±–∞ –ø–æ–∫—Ä—ã–≤–∞—é—Ç {top_two_share:.1f}% —Ä—ã–Ω–∫–∞\n"
            elif top_two_share > 50:
                result += f"‚Ä¢ –£–º–µ—Ä–µ–Ω–Ω–∞—è –∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—è: –¥–≤–∞ –ª–∏–¥–µ—Ä–∞ –∑–∞–Ω–∏–º–∞—é—Ç {top_two_share:.1f}% —Ä—ã–Ω–∫–∞\n"
            else:
                result += f"‚Ä¢ –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ–∂–¥—É –≤—Å–µ–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏ –æ–ø–ª–∞—Ç—ã\n"
        
        return result
    
    def _analyze_multifactor(self, data_analysis: Dict[str, Any]) -> str:
        """–ú–Ω–æ–≥–æ—Ñ–∞–∫—Ç–æ—Ä–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–æ–≤–∏—á–∫–æ–≤ vs —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º (–í–æ–ø—Ä–æ—Å 8)"""
        
        multifactor_data = data_analysis.get("multifactor_earnings", {})
        regional_differences = data_analysis.get("regional_skill_differences", {})
        summary = data_analysis.get("summary", {})
        
        if not multifactor_data and not regional_differences:
            return "–î–∞–Ω–Ω—ã–µ –¥–ª—è –º–Ω–æ–≥–æ—Ñ–∞–∫—Ç–æ—Ä–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."
        
        result = f"""üîç **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–æ—Ö–æ–¥–æ–≤ –Ω–æ–≤–∏—á–∫–æ–≤ –∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º:**

üìä **–ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞** ({summary.get('total_records', 0)} —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤):
‚Ä¢ –û–±—â–∏–π —Å—Ä–µ–¥–Ω–∏–π –¥–æ—Ö–æ–¥: ${summary.get('mean_earnings', 0):.2f}

üåç **–†–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è:**
"""
        
        if regional_differences:
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ä–µ–≥–∏–æ–Ω—ã –ø–æ —Ä–∞–∑–Ω–∏—Ü–µ –º–µ–∂–¥—É —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ –∏ –Ω–æ–≤–∏—á–∫–∞–º–∏
            sorted_regions = sorted(
                regional_differences.items(), 
                key=lambda x: x[1].get('difference', 0), 
                reverse=True
            )
            
            for region, stats in sorted_regions[:8]:  # –¢–æ–ø 8 —Ä–µ–≥–∏–æ–Ω–æ–≤
                expert_avg = stats.get('expert_avg', 0)
                beginner_avg = stats.get('beginner_avg', 0)
                difference = stats.get('difference', 0)
                
                if expert_avg > 0 and beginner_avg > 0:
                    percentage_diff = (difference / beginner_avg * 100) if beginner_avg > 0 else 0
                    
                    result += f"**{region}:**\n"
                    result += f"‚Ä¢ –≠–∫—Å–ø–µ—Ä—Ç—ã: ${expert_avg:.2f}\n"
                    result += f"‚Ä¢ –ù–æ–≤–∏—á–∫–∏: ${beginner_avg:.2f}\n"
                    result += f"‚Ä¢ –†–∞–∑–Ω–∏—Ü–∞: ${difference:.2f} ({percentage_diff:+.1f}%)\n\n"
        
        # –û–±—â–∏–µ –≤—ã–≤–æ–¥—ã
        if regional_differences:
            avg_difference = sum(stats.get('difference', 0) for stats in regional_differences.values()) / len(regional_differences)
            max_diff_region = max(regional_differences.items(), key=lambda x: x[1].get('difference', 0))
            min_diff_region = min(regional_differences.items(), key=lambda x: x[1].get('difference', 0))
            
            result += f"üí° **–ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã:**\n"
            result += f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è —Ä–∞–∑–Ω–∏—Ü–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏ –Ω–æ–≤–∏—á–∫–æ–≤: ${avg_difference:.2f}\n"
            result += f"‚Ä¢ –ù–∞–∏–±–æ–ª—å—à–∏–π —Ä–∞–∑—Ä—ã–≤ –≤ {max_diff_region[0]}: ${max_diff_region[1].get('difference', 0):.2f}\n"
            result += f"‚Ä¢ –ù–∞–∏–º–µ–Ω—å—à–∏–π —Ä–∞–∑—Ä—ã–≤ –≤ {min_diff_region[0]}: ${min_diff_region[1].get('difference', 0):.2f}\n"
            
            if avg_difference > 1000:
                result += "‚Ä¢ –°—É—â–µ—Å—Ç–≤–µ–Ω–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –≤ –¥–æ—Ö–æ–¥–∞—Ö –º–µ–∂–¥—É —É—Ä–æ–≤–Ω—è–º–∏ –æ–ø—ã—Ç–∞ –≤–æ –≤—Å–µ—Ö —Ä–µ–≥–∏–æ–Ω–∞—Ö"
            else:
                result += "‚Ä¢ –£–º–µ—Ä–µ–Ω–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞ –≤ –¥–æ—Ö–æ–¥–∞—Ö –º–µ–∂–¥—É –Ω–æ–≤–∏—á–∫–∞–º–∏ –∏ —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏"
        
        return result
    
    def _analyze_hourly_rates(self, data_analysis: Dict[str, Any]) -> str:
        """–ò–°–ü–†–ê–í–õ–ï–ù–û: –ê–Ω–∞–ª–∏–∑ –ø–æ—á–∞—Å–æ–≤—ã—Ö —Å—Ç–∞–≤–æ–∫ –ø–æ —Å–ø–æ—Å–æ–±–∞–º –æ–ø–ª–∞—Ç—ã (–í–æ–ø—Ä–æ—Å 9)"""
        
        hourly_rate_data = data_analysis.get("hourly_rate_by_payment", {})
        summary = data_analysis.get("summary", {})
        
        if not hourly_rate_data:
            return "–î–∞–Ω–Ω—ã–µ –æ –ø–æ—á–∞—Å–æ–≤—ã—Ö —Å—Ç–∞–≤–∫–∞—Ö –ø–æ —Å–ø–æ—Å–æ–±–∞–º –æ–ø–ª–∞—Ç—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."
        
        result = f"""üí∞ **–°—Ä–µ–¥–Ω—è—è –ø–æ—á–∞—Å–æ–≤–∞—è —Å—Ç–∞–≤–∫–∞ —É —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤ —Å —Ä–∞–∑–Ω—ã–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏ –æ–ø–ª–∞—Ç—ã:**

üìä **–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞** ({summary.get('total_records', 0)} —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤):
‚Ä¢ –û–±—â–∏–π —Å—Ä–µ–¥–Ω–∏–π –¥–æ—Ö–æ–¥: ${summary.get('mean_earnings', 0):.2f}

‚è∞ **–ü–æ—á–∞—Å–æ–≤—ã–µ —Å—Ç–∞–≤–∫–∏ –ø–æ —Å–ø–æ—Å–æ–±–∞–º –æ–ø–ª–∞—Ç—ã:**
"""
        
        if "mean" in hourly_rate_data:
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø–æ—á–∞—Å–æ–≤–æ–π —Å—Ç–∞–≤–∫–µ
            sorted_methods = sorted(hourly_rate_data["mean"].items(), key=lambda x: x[1], reverse=True)
            
            for i, (method, rate) in enumerate(sorted_methods, 1):
                count = hourly_rate_data.get("count", {}).get(method, 0)
                median_rate = hourly_rate_data.get("median", {}).get(method, rate)
                
                emoji = "üèÜ" if i == 1 else "ü•á" if i == 2 else "ü•à" if i == 3 else "üî∏"
                
                if not pd.isna(rate):
                    result += f"{emoji} **{method}**: ${rate:.2f}/—á–∞—Å (–º–µ–¥–∏–∞–Ω–∞: ${median_rate:.2f}) ‚Äî {count} —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤\n"
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–∏–π –≤ —Å—Ç–∞–≤–∫–∞—Ö
        if "mean" in hourly_rate_data and len(hourly_rate_data["mean"]) >= 2:
            rates = [v for v in hourly_rate_data["mean"].values() if not pd.isna(v)]
            if rates:
                highest_rate = max(rates)
                lowest_rate = min(rates)
                rate_spread = highest_rate - lowest_rate
                
                result += f"\nüí° **–ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã:**\n"
                result += f"‚Ä¢ –ù–∞–∏–≤—ã—Å—à–∞—è —Å—Ç–∞–≤–∫–∞: ${highest_rate:.2f}/—á–∞—Å\n"
                result += f"‚Ä¢ –ù–∞–∏–º–µ–Ω—å—à–∞—è —Å—Ç–∞–≤–∫–∞: ${lowest_rate:.2f}/—á–∞—Å\n"
                result += f"‚Ä¢ –†–∞–∑–±—Ä–æ—Å —Å—Ç–∞–≤–æ–∫: ${rate_spread:.2f}/—á–∞—Å\n"
                
                if rate_spread > 10:
                    result += "‚Ä¢ –°–ø–æ—Å–æ–± –æ–ø–ª–∞—Ç—ã —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –ø–æ—á–∞—Å–æ–≤—É—é —Å—Ç–∞–≤–∫—É\n"
                    result += "‚Ä¢ –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –º–µ—Ç–æ–¥—ã –æ–ø–ª–∞—Ç—ã –ø–æ–∑–≤–æ–ª—è—é—Ç —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–∏–µ —Å—Ç–∞–≤–∫–∏"
                else:
                    result += "‚Ä¢ –°–ø–æ—Å–æ–± –æ–ø–ª–∞—Ç—ã —Å–ª–∞–±–æ –≤–ª–∏—è–µ—Ç –Ω–∞ –ø–æ—á–∞—Å–æ–≤—É—é —Å—Ç–∞–≤–∫—É\n"
                    result += "‚Ä¢ –°—Ç–∞–≤–∫–∏ –ø—Ä–∏–º–µ—Ä–Ω–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã –¥–ª—è –≤—Å–µ—Ö –º–µ—Ç–æ–¥–æ–≤ –æ–ø–ª–∞—Ç—ã"
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
                avg_rate = sum(rates) / len(rates)
                result += f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è –ø–æ—á–∞—Å–æ–≤–∞—è —Å—Ç–∞–≤–∫–∞ –ø–æ —Ä—ã–Ω–∫—É: ${avg_rate:.2f}/—á–∞—Å\n"
                
                # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
                best_method = max(hourly_rate_data["mean"].items(), key=lambda x: x[1])[0]
                result += f"‚Ä¢ –ù–∞–∏–ª—É—á—à–∏–π —Å–ø–æ—Å–æ–± –æ–ø–ª–∞—Ç—ã –ø–æ —Å—Ç–∞–≤–∫–µ: **{best_method}**"
        
        return result
    
    def _analyze_activity_by_qualification(self, data_analysis: Dict[str, Any]) -> str:
        """–ê–Ω–∞–ª–∏–∑ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ —É—Ä–æ–≤–Ω—é –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏ (–í–æ–ø—Ä–æ—Å 10)"""
        
        activity_data = data_analysis.get("activity_by_qualification", {})
        summary = data_analysis.get("summary", {})
        
        if not activity_data:
            return "–î–∞–Ω–Ω—ã–µ –æ–± –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ —É—Ä–æ–≤–Ω—é –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."
        
        result = f"""üìã **–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–µ–∫—Ç–æ–≤ –ø–æ —É—Ä–æ–≤–Ω—é –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏:**

üìä **–û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞** ({summary.get('total_records', 0)} —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤):

**–ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ —É—Ä–æ–≤–Ω—è–º –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏:**
"""
        
        if "mean" in activity_data:
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø—Ä–æ–µ–∫—Ç–æ–≤
            sorted_skills = sorted(activity_data["mean"].items(), key=lambda x: x[1], reverse=True)
            
            for skill, avg_projects in sorted_skills:
                count = activity_data.get("count", {}).get(skill, 0)
                median_projects = activity_data.get("median", {}).get(skill, avg_projects)
                std_projects = activity_data.get("std", {}).get(skill, 0)
                
                if not pd.isna(avg_projects):
                    result += f"‚Ä¢ **{skill.capitalize()}**: {avg_projects:.1f} –ø—Ä–æ–µ–∫—Ç–æ–≤ –≤ —Å—Ä–µ–¥–Ω–µ–º\n"
                    result += f"  ‚îî –ú–µ–¥–∏–∞–Ω–∞: {median_projects:.1f}, –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: {std_projects:.1f}, –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ: {count}\n\n"
        
        # –ê–Ω–∞–ª–∏–∑ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π
        if "mean" in activity_data and len(activity_data["mean"]) >= 2:
            projects_by_skill = {k: v for k, v in activity_data["mean"].items() if not pd.isna(v)}
            
            if projects_by_skill:
                most_active = max(projects_by_skill.items(), key=lambda x: x[1])
                least_active = min(projects_by_skill.items(), key=lambda x: x[1])
                
                result += f"üí° **–ö–ª—é—á–µ–≤—ã–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è:**\n"
                result += f"‚Ä¢ –ù–∞–∏–±–æ–ª–µ–µ –∞–∫—Ç–∏–≤–Ω—ã–µ: **{most_active[0]}** ({most_active[1]:.1f} –ø—Ä–æ–µ–∫—Ç–æ–≤)\n"
                result += f"‚Ä¢ –ù–∞–∏–º–µ–Ω–µ–µ –∞–∫—Ç–∏–≤–Ω—ã–µ: **{least_active[0]}** ({least_active[1]:.1f} –ø—Ä–æ–µ–∫—Ç–æ–≤)\n"
                
                activity_ratio = most_active[1] / least_active[1] if least_active[1] > 0 else 0
                result += f"‚Ä¢ –†–∞–∑–Ω–∏—Ü–∞ –≤ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: {activity_ratio:.1f}x\n"
                
                # –õ–æ–≥–∏—á–µ—Å–∫–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
                if 'expert' in projects_by_skill and 'beginner' in projects_by_skill:
                    expert_projects = projects_by_skill['expert']
                    beginner_projects = projects_by_skill['beginner']
                    
                    if expert_projects > beginner_projects:
                        result += "‚Ä¢ –≠–∫—Å–ø–µ—Ä—Ç—ã –±–æ–ª–µ–µ –∞–∫—Ç–∏–≤–Ω—ã –∏ –±–µ—Ä—É—Ç—Å—è –∑–∞ –±–æ–ª—å—à–µ –ø—Ä–æ–µ–∫—Ç–æ–≤\n"
                    else:
                        result += "‚Ä¢ –ù–æ–≤–∏—á–∫–∏ –±–æ–ª–µ–µ –∞–∫—Ç–∏–≤–Ω—ã, –≤–æ–∑–º–æ–∂–Ω–æ, –Ω–∞–±–∏—Ä–∞—é—Ç –æ–ø—ã—Ç\n"
        
        return result
    
    def _interpret_correlation(self, correlation: float) -> str:
        """–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏"""
        if abs(correlation) >= 0.7:
            return "–°–∏–ª—å–Ω–∞—è —Å–≤—è–∑—å"
        elif abs(correlation) >= 0.3:
            return "–£–º–µ—Ä–µ–Ω–Ω–∞—è —Å–≤—è–∑—å"
        elif abs(correlation) >= 0.1:
            return "–°–ª–∞–±–∞—è —Å–≤—è–∑—å"
        else:
            return "–û—á–µ–Ω—å —Å–ª–∞–±–∞—è —Å–≤—è–∑—å"
    
    def _analyze_expert_percentage(self, data_analysis: Dict[str, Any]) -> str:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ—Ü–µ–Ω—Ç–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤"""
        
        if "total_experts" in data_analysis:
            total_experts = data_analysis.get("total_experts", 0)
            experts_under_100 = data_analysis.get("experts_under_100_projects", 0)
            percentage = data_analysis.get("percentage_under_100_projects", 0)
            
            result = f"""üìä **–ê–Ω–∞–ª–∏–∑ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ø—Ä–æ–µ–∫—Ç–æ–≤:**

üë• **–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤**: {total_experts}
üìà **–≠–∫—Å–ø–µ—Ä—Ç—ã —Å –º–µ–Ω–µ–µ —á–µ–º 100 –ø—Ä–æ–µ–∫—Ç–∞–º–∏**: {experts_under_100}
üìä **–ü—Ä–æ—Ü–µ–Ω—Ç —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ —Å <100 –ø—Ä–æ–µ–∫—Ç–∞–º–∏**: {percentage:.1f}%
üéØ **–≠–∫—Å–ø–µ—Ä—Ç—ã —Å–æ 100+ –ø—Ä–æ–µ–∫—Ç–∞–º–∏**: {total_experts - experts_under_100} ({100-percentage:.1f}%)

üí° **–í—ã–≤–æ–¥**: """
            
            if percentage > 70:
                result += "–ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏–º–µ–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –Ω–µ–±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–µ–∫—Ç–æ–≤, —á—Ç–æ —É–∫–∞–∑—ã–≤–∞–µ—Ç –Ω–∞ –≤–∞–∂–Ω–æ—Å—Ç—å –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç—ã –Ω–∞–¥ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º."
            elif percentage > 50:
                result += "–ü—Ä–∏–º–µ—Ä–Ω–æ –ø–æ–ª–æ–≤–∏–Ω–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏–º–µ–µ—Ç –º–µ–Ω–µ–µ 100 –ø—Ä–æ–µ–∫—Ç–æ–≤, —á—Ç–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø—É—Ç–µ–π –∫ —ç–∫—Å–ø–µ—Ä—Ç–∏–∑–µ."
            else:
                result += "–ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏–º–µ–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–π –æ–ø—ã—Ç —Å 100+ –ø—Ä–æ–µ–∫—Ç–∞–º–∏, —á—Ç–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω–æ–≥–æ –æ–ø—ã—Ç–∞."
            
            return result
        
        return "–î–∞–Ω–Ω—ã–µ –æ–± —ç–∫—Å–ø–µ—Ä—Ç–∞—Ö –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞."
    
    def _analyze_relationships(self, data_analysis: Dict[str, Any], query: str) -> str:
        """–ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ñ–∞–∫—Ç–æ—Ä–∞–º–∏"""
        
        skills_data = data_analysis.get("experience_earnings", {})
        payment_data = data_analysis.get("earnings_by_payment_method", {})
        regional_data = data_analysis.get("regional_earnings", {})
        summary = data_analysis.get("summary", {})
        
        result = f"""üîó **–ê–Ω–∞–ª–∏–∑ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–µ–π –≤ –¥–∞–Ω–Ω—ã—Ö –æ —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–∞—Ö:**

üìä **–ë–∞–∑–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏** ({summary.get('total_records', 0)} —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤):
‚Ä¢ –°—Ä–µ–¥–Ω–∏–π –¥–æ—Ö–æ–¥: ${summary.get('mean_earnings', 0):.2f}
‚Ä¢ –ú–µ–¥–∏–∞–Ω–Ω—ã–π –¥–æ—Ö–æ–¥: ${summary.get('median_earnings', 0):.2f}

"""
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –Ω–∞–≤—ã–∫–∞–º –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
        if skills_data and "mean" in skills_data:
            result += "üí° **–í–ª–∏—è–Ω–∏–µ —É—Ä–æ–≤–Ω—è –Ω–∞–≤—ã–∫–æ–≤ –Ω–∞ –¥–æ—Ö–æ–¥—ã:**\n"
            sorted_skills = sorted(skills_data["mean"].items(), key=lambda x: x[1], reverse=True)
            
            highest_skill = sorted_skills[0] if sorted_skills else None
            lowest_skill = sorted_skills[-1] if sorted_skills else None
            
            for skill, income in sorted_skills:
                count = skills_data.get("count", {}).get(skill, 0)
                vs_average = ((income - summary.get('mean_earnings', 0)) / summary.get('mean_earnings', 1)) * 100
                result += f"‚Ä¢ **{skill.capitalize()}**: ${income:.2f} ({vs_average:+.1f}% –∫ —Å—Ä–µ–¥–Ω–µ–º—É) ‚Äî {count} —á–µ–ª.\n"
            
            if highest_skill and lowest_skill:
                skill_diff = highest_skill[1] - lowest_skill[1]
                result += f"\nüìà **–†–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –≤—ã—Å—à–∏–º –∏ –Ω–∏–∑—à–∏–º —É—Ä–æ–≤–Ω–µ–º**: ${skill_diff:.2f}\n"
        
        # –ê–Ω–∞–ª–∏–∑ —Å–ø–æ—Å–æ–±–æ–≤ –æ–ø–ª–∞—Ç—ã –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
        if payment_data and "mean" in payment_data:
            result += "\nüí≥ **–í–ª–∏—è–Ω–∏–µ —Å–ø–æ—Å–æ–±–∞ –æ–ø–ª–∞—Ç—ã –Ω–∞ –¥–æ—Ö–æ–¥—ã:**\n"
            sorted_payments = sorted(payment_data["mean"].items(), key=lambda x: x[1], reverse=True)
            
            for method, income in sorted_payments:
                count = payment_data.get("count", {}).get(method, 0)
                vs_average = ((income - summary.get('mean_earnings', 0)) / summary.get('mean_earnings', 1)) * 100
                result += f"‚Ä¢ **{method}**: ${income:.2f} ({vs_average:+.1f}% –∫ —Å—Ä–µ–¥–Ω–µ–º—É) ‚Äî {count} —á–µ–ª.\n"
        
        # –†–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
        if regional_data and "mean" in regional_data:
            result += "\nüåç **–†–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è:**\n"
            sorted_regions = sorted(regional_data["mean"].items(), key=lambda x: x[1], reverse=True)
            
            top_region = sorted_regions[0] if sorted_regions else None
            bottom_region = sorted_regions[-1] if sorted_regions else None
            
            if top_region and bottom_region:
                regional_diff = top_region[1] - bottom_region[1]
                result += f"‚Ä¢ –ù–∞–∏–±–æ–ª—å—à–∏–µ –¥–æ—Ö–æ–¥—ã: **{top_region[0]}** (${top_region[1]:.2f})\n"
                result += f"‚Ä¢ –ù–∞–∏–º–µ–Ω—å—à–∏–µ –¥–æ—Ö–æ–¥—ã: **{bottom_region[0]}** (${bottom_region[1]:.2f})\n"
                result += f"‚Ä¢ –†–µ–≥–∏–æ–Ω–∞–ª—å–Ω–∞—è —Ä–∞–∑–Ω–∏—Ü–∞: ${regional_diff:.2f}\n"
        
        # –í—ã–≤–æ–¥—ã
        result += "\nüéØ **–ö–ª—é—á–µ–≤—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏:**\n"
        
        if skills_data and payment_data:
            result += "‚Ä¢ –£—Ä–æ–≤–µ–Ω—å –Ω–∞–≤—ã–∫–æ–≤ –∏ —Å–ø–æ—Å–æ–± –æ–ø–ª–∞—Ç—ã —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤–ª–∏—è—é—Ç –Ω–∞ –¥–æ—Ö–æ–¥—ã\n"
            result += "‚Ä¢ –≠–∫—Å–ø–µ—Ä—Ç—ã –º–æ–≥—É—Ç –≤—ã–±–∏—Ä–∞—Ç—å –±–æ–ª–µ–µ –≤—ã–≥–æ–¥–Ω—ã–µ —É—Å–ª–æ–≤–∏—è —Ä–∞–±–æ—Ç—ã\n"
            result += "‚Ä¢ –ö–æ–º–±–∏–Ω–∞—Ü–∏—è –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è –Ω–∞–≤—ã–∫–æ–≤ –∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ —Å–ø–æ—Å–æ–±–∞ –æ–ø–ª–∞—Ç—ã –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç –¥–æ—Ö–æ–¥—ã\n"
        
        return result
    
    def _analyze_crypto_comparison(self, data_analysis: Dict[str, Any]) -> str:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–æ–π"""
        
        earnings_data = data_analysis.get("earnings_by_payment_method", {})
        summary = data_analysis.get("summary", {})
        
        if not earnings_data or "mean" not in earnings_data:
            return "–î–∞–Ω–Ω—ã–µ –æ –¥–æ—Ö–æ–¥–∞—Ö –ø–æ —Å–ø–æ—Å–æ–±–∞–º –æ–ø–ª–∞—Ç—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."
        
        methods = earnings_data["mean"]
        counts = earnings_data.get("count", {})
        
        # –ù–∞—Ö–æ–¥–∏–º –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç—É
        crypto_method = None
        crypto_income = 0
        for method, income in methods.items():
            if "crypto" in method.lower():
                crypto_method = method
                crypto_income = income
                break
        
        if not crypto_method:
            return "–î–∞–Ω–Ω—ã–µ –æ –¥–æ—Ö–æ–¥–∞—Ö —Å –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–æ–π –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ."
        
        result = f"""üí∞ **–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–æ—Ö–æ–¥–æ–≤ —Å –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–æ–π:**

üî∏ **{crypto_method}**: ${crypto_income:.2f} (—Å—Ä–µ–¥–Ω–µ–µ) ‚Äî {counts.get(crypto_method, 0)} —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤

**–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏ –æ–ø–ª–∞—Ç—ã:**
"""
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏
        other_methods = [(method, income) for method, income in methods.items() if method != crypto_method]
        other_methods.sort(key=lambda x: x[1], reverse=True)
        
        for method, income in other_methods:
            diff = crypto_income - income
            percentage = (diff / income * 100) if income > 0 else 0
            count = counts.get(method, 0)
            
            if diff > 0:
                comp_text = f"–Ω–∞ ${diff:.2f} ({percentage:.1f}%) –≤—ã—à–µ"
                emoji = "üìà"
            elif diff < 0:
                comp_text = f"–Ω–∞ ${abs(diff):.2f} ({abs(percentage):.1f}%) –Ω–∏–∂–µ"
                emoji = "üìâ"
            else:
                comp_text = "–ø—Ä–∏–º–µ—Ä–Ω–æ –æ–¥–∏–Ω–∞–∫–æ–≤–æ"
                emoji = "‚öñÔ∏è"
            
            result += f"{emoji} **{method}**: ${income:.2f} ‚Äî {count} —á–µ–ª. ({comp_text})\n"
        
        # –û–±—â–∏–π –≤—ã–≤–æ–¥
        avg_other = sum(income for _, income in other_methods) / len(other_methods) if other_methods else 0
        diff_vs_avg = crypto_income - avg_other
        
        result += f"\nüí° **–í—ã–≤–æ–¥**: –§—Ä–∏–ª–∞–Ω—Å–µ—Ä—ã —Å –∫—Ä–∏–ø—Ç–æ–æ–ø–ª–∞—Ç–æ–π –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç "
        if diff_vs_avg > 0:
            result += f"–≤ —Å—Ä–µ–¥–Ω–µ–º –Ω–∞ ${diff_vs_avg:.2f} –±–æ–ª—å—à–µ –¥—Ä—É–≥–∏—Ö —Å–ø–æ—Å–æ–±–æ–≤ –æ–ø–ª–∞—Ç—ã."
        else:
            result += f"–≤ —Å—Ä–µ–¥–Ω–µ–º –Ω–∞ ${abs(diff_vs_avg):.2f} –º–µ–Ω—å—à–µ –¥—Ä—É–≥–∏—Ö —Å–ø–æ—Å–æ–±–æ–≤ –æ–ø–ª–∞—Ç—ã."
        
        return result
    
    def _analyze_regional_distribution(self, data_analysis: Dict[str, Any]) -> str:
        """–ê–Ω–∞–ª–∏–∑ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"""
        
        regional_data = data_analysis.get("regional_earnings", {})
        summary = data_analysis.get("summary", {})
        
        if not regional_data or "mean" not in regional_data:
            return "–î–∞–Ω–Ω—ã–µ –æ —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."
        
        regions = regional_data["mean"]
        counts = regional_data.get("count", {})
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é –¥–æ—Ö–æ–¥–∞
        sorted_regions = sorted(regions.items(), key=lambda x: x[1], reverse=True)
        
        result = f"""üåç **–†–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ—Ö–æ–¥–æ–≤ —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤:**

üìä **–°—Ä–µ–¥–Ω–∏–π –¥–æ—Ö–æ–¥ –ø–æ —Ä—ã–Ω–∫—É**: ${summary.get('mean_earnings', 0):.2f}

**–†–µ–π—Ç–∏–Ω–≥ —Ä–µ–≥–∏–æ–Ω–æ–≤ –ø–æ –¥–æ—Ö–æ–¥–∞–º:**
"""
        
        for i, (region, income) in enumerate(sorted_regions[:8], 1):
            count = counts.get(region, 0)
            vs_avg = income - summary.get('mean_earnings', 0)
            emoji = "ü•á" if i == 1 else "ü•à" if i == 2 else "ü•â" if i == 3 else "üî∏"
            
            result += f"{emoji} **{region}**: ${income:.2f} ({vs_avg:+.0f}$ –∫ —Å—Ä–µ–¥–Ω–µ–º—É) ‚Äî {count} —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤\n"
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–±—Ä–æ—Å–∞
        if len(sorted_regions) >= 2:
            highest = sorted_regions[0][1]
            lowest = sorted_regions[-1][1]
            spread = highest - lowest
            
            result += f"\nüìà **–†–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑**:\n"
            result += f"‚Ä¢ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –¥–æ—Ö–æ–¥: {sorted_regions[0][0]} (${highest:.2f})\n"
            result += f"‚Ä¢ –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –¥–æ—Ö–æ–¥: {sorted_regions[-1][0]} (${lowest:.2f})\n"
            result += f"‚Ä¢ –†–∞–∑–±—Ä–æ—Å –¥–æ—Ö–æ–¥–æ–≤: ${spread:.2f} ({(spread/lowest*100):.1f}%)\n"
        
        return result
    
    def _generate_general_analysis(self, data_analysis: Dict[str, Any], query: str) -> str:
        """–û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –Ω–µ—Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        
        summary = data_analysis.get("summary", {})
        
        result = f"""üìä **–û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –æ —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–∞—Ö:**

üî¢ **–û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏**:
‚Ä¢ –í—Å–µ–≥–æ —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤ –≤ –≤—ã–±–æ—Ä–∫–µ: {summary.get('total_records', 0)}
‚Ä¢ –°—Ä–µ–¥–Ω–∏–π –¥–æ—Ö–æ–¥: ${summary.get('mean_earnings', 0):.2f}
‚Ä¢ –ú–µ–¥–∏–∞–Ω–Ω—ã–π –¥–æ—Ö–æ–¥: ${summary.get('median_earnings', 0):.2f}
‚Ä¢ –î–∏–∞–ø–∞–∑–æ–Ω –¥–æ—Ö–æ–¥–æ–≤: ${summary.get('min_earnings', 0):.2f} - ${summary.get('max_earnings', 0):.2f}
"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∞–Ω–∞–ª–∏–∑—ã
        available_analyses = []
        
        if "earnings_by_payment_method" in data_analysis:
            available_analyses.append("–∞–Ω–∞–ª–∏–∑ –ø–æ —Å–ø–æ—Å–æ–±–∞–º –æ–ø–ª–∞—Ç—ã")
        
        if "regional_earnings" in data_analysis:
            available_analyses.append("—Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑")
        
        if "experience_earnings" in data_analysis:
            available_analyses.append("–∞–Ω–∞–ª–∏–∑ –ø–æ —É—Ä–æ–≤–Ω—é –Ω–∞–≤—ã–∫–æ–≤")
        
        if "total_experts" in data_analysis:
            available_analyses.append("—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤")
        
        if available_analyses:
            result += f"\nüìã **–î–æ—Å—Ç—É–ø–Ω—ã–µ –≤–∏–¥—ã –∞–Ω–∞–ª–∏–∑–∞**: {', '.join(available_analyses)}\n"
        
        result += f"""\nüí° **–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞**:
‚Ä¢ –ó–∞–¥–∞–π—Ç–µ –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å –æ —Ñ–∞–∫—Ç–æ—Ä–∞—Ö, –∫–æ—Ç–æ—Ä—ã–µ –≤–∞—Å –∏–Ω—Ç–µ—Ä–µ—Å—É—é—Ç
‚Ä¢ –°–ø—Ä–æ—Å–∏—Ç–µ –æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ –≥—Ä—É–ø–ø —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤
‚Ä¢ –£—Ç–æ—á–Ω–∏—Ç–µ –∏–Ω—Ç–µ—Ä–µ—Å—É—é—â—É—é –≤–∞—Å –º–µ—Ç—Ä–∏–∫—É (–¥–æ—Ö–æ–¥—ã, –ø—Ä–æ–µ–∫—Ç—ã, —Ä–µ–≥–∏–æ–Ω—ã)

üéØ **–ü—Ä–∏–º–µ—Ä—ã –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤**:
‚Ä¢ "–ö–∞–∫–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ –≤–ª–∏—è—é—Ç –Ω–∞ –¥–æ—Ö–æ–¥—ã?"
‚Ä¢ "–°—Ä–∞–≤–Ω–∏—Ç–µ –¥–æ—Ö–æ–¥—ã —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏ –Ω–æ–≤–∏—á–∫–æ–≤"
‚Ä¢ "–ö–∞–∫–∞—è —Å–≤—è–∑—å –º–µ–∂–¥—É —Ä–µ–≥–∏–æ–Ω–æ–º –∏ —Å–ø–æ—Å–æ–±–æ–º –æ–ø–ª–∞—Ç—ã?"
"""
        
        return result
    
    def get_device_info(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ"""
        info = {
            "device": self.device,
            "model_name": self.model_name,
            "cuda_available": torch.cuda.is_available(),
            "model_loaded": self.pipeline is not None,
            "ready": self.ready
        }
        
        if torch.cuda.is_available():
            info.update({
                "gpu_count": torch.cuda.device_count(),
                "gpu_name": torch.cuda.get_device_name(0) if torch.cuda.device_count() > 0 else None,
                "gpu_memory_total": torch.cuda.get_device_properties(0).total_memory / (1024**3) if torch.cuda.device_count() > 0 else 0
            })
            
            if self.device == "cuda":
                info.update({
                    "gpu_memory_allocated": torch.cuda.memory_allocated(0) / (1024**3),
                    "gpu_memory_reserved": torch.cuda.memory_reserved(0) / (1024**3)
                })
        
        return info